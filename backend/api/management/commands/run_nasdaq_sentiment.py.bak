"""
NASDAQ Composite Sentiment Tracker - Hybrid Approach
Tracks sentiment across top 20 NASDAQ stocks + general market news
Run with: python manage.py run_nasdaq_sentiment
"""

import os
import time
import hashlib
from datetime import datetime, timedelta
from decimal import Decimal
from django.core.management.base import BaseCommand
from django.utils import timezone
from django.db.models import Avg, Min, Max, Count, Sum
from django.db import transaction
import requests
import finnhub
from dotenv import load_dotenv

# Load environment variables from project root
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..', '.env'))

# Import models
from api.models import Ticker, AnalysisRun, NewsArticle, SentimentHistory, TickerContribution

# Import NASDAQ configuration
from .nasdaq_config import (
    NASDAQ_TOP_20,
    COMPANY_NAMES,
    MARKET_MOVING_KEYWORDS,
    EXCLUDE_KEYWORDS,
    SOURCE_CREDIBILITY,
    SENTIMENT_WEIGHTS,
    API_RATE_LIMIT_DELAY,
    INDICATOR_SYMBOLS,
    INDICATOR_RESOLUTION,
    INDICATOR_LOOKBACK_HOURS,
    INDICATOR_PERIODS,
)

# Import technical indicators calculator (uses Yahoo Finance OHLCV + math)
from .technical_indicators import fetch_indicators_with_fallback, fetch_latest_ohlcv_from_yfinance, calculate_technical_composite_score

# Import Reddit sentiment analysis
from .reddit_fetcher import fetch_all_reddit_content
from .reddit_sentiment_analyzer import analyze_reddit_content_batch

# ============================================================================
# CONFIGURATION
# ============================================================================

FINNHUB_API_KEY = os.environ.get('FINNHUB_API_KEY', '')
HUGGINGFACE_API_KEY = os.environ.get('HUGGINGFACE_API_KEY', '')

# Weights for composite score calculation (within each article)
ARTICLE_WEIGHTS = {
    'base_sentiment': 0.40,
    'surprise_factor': 0.25,
    'novelty': 0.15,
    'source_credibility': 0.10,
    'recency': 0.10
}

# Cache to track seen articles in current session
seen_articles = set()

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_article_hash(headline, summary):
    """Generate unique hash for article deduplication"""
    content = f"{headline}{summary}".lower()
    return hashlib.md5(content.encode()).hexdigest()


def get_cached_sentiment_from_db(article_hash):
    """Get cached sentiment from database"""
    try:
        article = NewsArticle.objects.filter(article_hash=article_hash).first()
        if article and article.is_analyzed:
            return article.base_sentiment
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching cached sentiment: {e}")
    return None


def analyze_sentiment_finbert_api(text):
    """Analyze sentiment using FinBERT via HuggingFace API (single text)"""
    API_URL = "https://api-inference.huggingface.co/models/ProsusAI/finbert"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
    text = text[:512]
    
    try:
        response = requests.post(API_URL, headers=headers, json={"inputs": text})
        
        if response.status_code == 200:
            result = response.json()
            sentiment_map = {'positive': 0, 'negative': 0, 'neutral': 0}
            
            for item in result[0]:
                label = item['label'].lower()
                score = item['score']
                sentiment_map[label] = score
            
            sentiment_score = sentiment_map['positive'] - sentiment_map['negative']
            return sentiment_score
            
        elif response.status_code == 503:
            print("  ‚è≥ Model loading, retrying in 20 seconds...")
            time.sleep(20)
            return analyze_sentiment_finbert_api(text)
        else:
            print(f"  ‚ö†Ô∏è API error: {response.status_code}")
            return 0.0
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error analyzing sentiment: {e}")
        return 0.0


def analyze_sentiment_finbert_batch(texts):
    """
    Analyze sentiment for multiple texts in a single batch API call
    Returns: list of sentiment scores in same order as input texts
    """
    API_URL = "https://api-inference.huggingface.co/models/ProsusAI/finbert"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}
    
    # Truncate each text to 512 characters
    truncated_texts = [text[:512] for text in texts]
    
    try:
        # Send batch request
        response = requests.post(API_URL, headers=headers, json={"inputs": truncated_texts})
        
        if response.status_code == 200:
            results = response.json()
            sentiment_scores = []
            
            # Process each result
            for result in results:
                sentiment_map = {'positive': 0, 'negative': 0, 'neutral': 0}
                
                for item in result:
                    label = item['label'].lower()
                    score = item['score']
                    sentiment_map[label] = score
                
                sentiment_score = sentiment_map['positive'] - sentiment_map['negative']
                sentiment_scores.append(sentiment_score)
            
            return sentiment_scores
            
        elif response.status_code == 503:
            print("  ‚è≥ Model loading, retrying in 20 seconds...")
            time.sleep(20)
            return analyze_sentiment_finbert_batch(texts)
        else:
            print(f"  ‚ö†Ô∏è Batch API error: {response.status_code}")
            # Return zeros for all texts
            return [0.0] * len(texts)
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è Error in batch sentiment analysis: {e}")
        return [0.0] * len(texts)


def calculate_surprise_factor(text):
    """Detect if news is surprising/unexpected"""
    text_lower = text.lower()
    
    surprise_keywords = {
        'unexpected': 1.5, 'surprise': 1.5, 'beats expectations': 1.8,
        'misses estimates': 1.8, 'exceeds expectations': 1.8, 'shock': 2.0,
        'unprecedented': 1.7, 'sudden': 1.4, 'abrupt': 1.4, 'breaking': 1.3
    }
    
    expected_keywords = {
        'as expected': 0.4, 'in line with': 0.4, 'anticipated': 0.5,
        'scheduled': 0.6, 'planned': 0.6
    }
    
    multiplier = 1.0
    for keyword, weight in surprise_keywords.items():
        if keyword in text_lower:
            multiplier = max(multiplier, weight)
    for keyword, weight in expected_keywords.items():
        if keyword in text_lower:
            multiplier = min(multiplier, weight)
    
    return multiplier


def calculate_novelty(article_hash):
    """Check if article is novel or duplicate"""
    if article_hash in seen_articles:
        return 0.2
    seen_articles.add(article_hash)
    return 1.0


def get_source_credibility(source):
    """Get credibility weight for news source"""
    for known_source, weight in SOURCE_CREDIBILITY.items():
        if known_source.lower() in source.lower():
            return weight
    return 0.5


def calculate_recency_weight(published_timestamp):
    """Calculate weight based on how recent the article is"""
    try:
        published_time = datetime.fromtimestamp(published_timestamp)
        hours_old = (datetime.now() - published_time).total_seconds() / 3600
        decay_factor = 2 ** (-hours_old / 6)
        return max(0, min(1.0, decay_factor))
    except:
        return 0.5


# ============================================================================
# NEWS FETCHING FUNCTIONS
# ============================================================================

def fetch_company_news_batch(finnhub_client, tickers, lookback_hours=24):
    """
    Fetch news for multiple tickers with rate limiting
    Returns: dict mapping ticker -> list of news articles
    """
    print(f"\nüì∞ Fetching company news for {len(tickers)} tickers...")
    
    to_date = datetime.now()
    from_date = to_date - timedelta(hours=lookback_hours)
    from_str = from_date.strftime('%Y-%m-%d')
    to_str = to_date.strftime('%Y-%m-%d')
    
    all_news = {}
    
    for ticker in tickers:
        try:
            print(f"  Fetching {ticker}...", end=' ')
            news = finnhub_client.company_news(ticker, _from=from_str, to=to_str)
            all_news[ticker] = news
            print(f"‚úì {len(news)} articles")
            
            # Rate limiting to avoid hitting API limits
            time.sleep(API_RATE_LIMIT_DELAY)
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            all_news[ticker] = []
    
    total_articles = sum(len(articles) for articles in all_news.values())
    print(f"‚úÖ Total company news articles fetched: {total_articles}")
    
    return all_news


def fetch_general_market_news(finnhub_client):
    """
    Fetch general market news and filter for market-moving content
    Returns: list of relevant market news articles
    """
    print(f"\nüìà Fetching general market news...")
    
    try:
        # Fetch general news - using 'general' category
        news = finnhub_client.general_news('general', min_id=0)
        
        print(f"  Received {len(news)} general news articles")
        
        # Filter for market-moving news
        relevant_news = []
        
        for article in news:
            headline = article.get('headline', '').lower()
            summary = article.get('summary', '').lower()
            combined_text = f"{headline} {summary}"
            
            # Check if article contains market-moving keywords
            has_market_keyword = any(
                keyword in combined_text 
                for keyword in MARKET_MOVING_KEYWORDS
            )
            
            # Check if article should be excluded (opinion pieces, etc.)
            has_exclude_keyword = any(
                keyword in combined_text 
                for keyword in EXCLUDE_KEYWORDS
            )
            
            if has_market_keyword and not has_exclude_keyword:
                relevant_news.append(article)
        
        print(f"‚úÖ Filtered to {len(relevant_news)} market-moving articles")
        return relevant_news
        
    except Exception as e:
        print(f"‚ùå Error fetching general market news: {e}")
        return []


# ============================================================================
# SENTIMENT ANALYSIS FUNCTIONS
# ============================================================================

def analyze_article_sentiment(article, ticker_obj, article_type='company', base_sentiment=None):
    """
    Analyze sentiment for a single article and return all metrics
    If base_sentiment is provided, skip FinBERT analysis (used in batch processing)
    Returns: dict with all sentiment components
    """
    headline = article.get('headline', '')
    summary = article.get('summary', '')
    source = article.get('source', 'Unknown')
    url = article.get('url', '')
    published_at = article.get('datetime', int(time.time()))
    
    # Generate unique hash
    article_hash = get_article_hash(headline, summary)
    
    # Check if already analyzed or if sentiment was provided
    if base_sentiment is None:
        cached_sentiment = get_cached_sentiment_from_db(article_hash)
        is_cached = cached_sentiment is not None
        
        # Analyze sentiment
        if is_cached:
            base_sentiment = cached_sentiment
        else:
            combined_text = f"{headline}. {summary}"
            base_sentiment = analyze_sentiment_finbert_api(combined_text)
    else:
        # Sentiment was provided from batch processing
        is_cached = False
    
    # Calculate all component factors
    surprise_factor = calculate_surprise_factor(f"{headline} {summary}")
    novelty_score = calculate_novelty(article_hash)
    source_credibility = get_source_credibility(source)
    recency_weight = calculate_recency_weight(published_at)
    
    # Calculate article score using weighted formula
    article_score = (
        base_sentiment * ARTICLE_WEIGHTS['base_sentiment'] * 100 +
        (surprise_factor - 1) * ARTICLE_WEIGHTS['surprise_factor'] * 50 +
        novelty_score * ARTICLE_WEIGHTS['novelty'] * 30 +
        source_credibility * ARTICLE_WEIGHTS['source_credibility'] * 20 +
        recency_weight * ARTICLE_WEIGHTS['recency'] * 20
    )
    
    return {
        'headline': headline,
        'summary': summary,
        'source': source,
        'url': url,
        'published_at': datetime.fromtimestamp(published_at),
        'article_hash': article_hash,
        'article_type': article_type,
        'base_sentiment': base_sentiment,
        'surprise_factor': surprise_factor,
        'novelty_score': novelty_score,
        'source_credibility': source_credibility,
        'recency_weight': recency_weight,
        'article_score': article_score,
        'is_cached': is_cached
    }


def analyze_articles_batch(articles, ticker_obj, article_type='company'):
    """
    Analyze sentiment for multiple articles using batch processing
    Returns: list of article data dicts
    """
    if not articles:
        return []
    
    # Separate cached and uncached articles
    articles_to_analyze = []
    cached_articles = []
    article_hashes = []
    
    for article in articles:
        headline = article.get('headline', '')
        summary = article.get('summary', '')
        article_hash = get_article_hash(headline, summary)
        article_hashes.append(article_hash)
        
        cached_sentiment = get_cached_sentiment_from_db(article_hash)
        if cached_sentiment is not None:
            # Article already analyzed
            cached_articles.append((article, cached_sentiment))
        else:
            # Need to analyze
            articles_to_analyze.append(article)
    
    # Batch process uncached articles
    new_sentiments = []
    if articles_to_analyze:
        print(f"    üî¨ Analyzing {len(articles_to_analyze)} new articles with FinBERT (batched)...")
        texts = [f"{a.get('headline', '')}. {a.get('summary', '')}" for a in articles_to_analyze]
        new_sentiments = analyze_sentiment_finbert_batch(texts)
    
    # Process all articles
    all_articles_data = []
    
    # Add cached articles
    for article, sentiment in cached_articles:
        article_data = analyze_article_sentiment(article, ticker_obj, article_type, base_sentiment=sentiment)
        all_articles_data.append(article_data)
    
    # Add newly analyzed articles
    for article, sentiment in zip(articles_to_analyze, new_sentiments):
        article_data = analyze_article_sentiment(article, ticker_obj, article_type, base_sentiment=sentiment)
        all_articles_data.append(article_data)
    
    if articles_to_analyze:
        print(f"    ‚úÖ Batch analysis complete ({len(cached_articles)} cached, {len(articles_to_analyze)} new)")
    
    return all_articles_data


def analyze_ticker_sentiment(finnhub_client, ticker_symbol, ticker_obj, lookback_hours=24):
    """
    Analyze sentiment for a single ticker's news
    Returns: tuple (sentiment_score, article_count, articles_data)
    """
    # Fetch news
    to_date = datetime.now()
    from_date = to_date - timedelta(hours=lookback_hours)
    from_str = from_date.strftime('%Y-%m-%d')
    to_str = to_date.strftime('%Y-%m-%d')
    
    try:
        news = finnhub_client.company_news(ticker_symbol, _from=from_str, to=to_str)
    except Exception as e:
        print(f"  ‚ùå Error fetching news for {ticker_symbol}: {e}")
        return 0.0, 0, []
    
    if not news:
        return 0.0, 0, []
    
    # Analyze all articles
    articles_data = []
    total_score = 0
    
    for article in news:
        article_data = analyze_article_sentiment(article, ticker_obj, article_type='company')
        articles_data.append(article_data)
        total_score += article_data['article_score']
    
    # Calculate average sentiment score
    avg_sentiment = total_score / len(articles_data) if articles_data else 0
    
    return avg_sentiment, len(articles_data), articles_data


# ============================================================================
# MAIN ANALYSIS FUNCTIONS
# ============================================================================

def run_nasdaq_composite_analysis(finnhub_client):
    """
    Main function to run complete NASDAQ composite analysis
    Uses hybrid approach: company news (70%) + market news (30%)
    """
    print("\n" + "="*80)
    print("üöÄ STARTING NASDAQ COMPOSITE SENTIMENT ANALYSIS")
    print("="*80)
    
    start_time = time.time()
    
    # Step 1: Initialize or get NASDAQ composite ticker
    nasdaq_ticker, created = Ticker.objects.get_or_create(
        symbol='^IXIC',
        defaults={
            'company_name': 'NASDAQ Composite Index',
            'exchange': 'NASDAQ'
        }
    )
    if created:
        print(f"‚ú® Created NASDAQ composite ticker: ^IXIC")
    
    # Step 2: Initialize all component tickers
    print(f"\nüìä Initializing {len(NASDAQ_TOP_20)} component tickers...")
    ticker_objects = {}
    for symbol, weight in NASDAQ_TOP_20.items():
        ticker_obj, _ = Ticker.objects.get_or_create(
            symbol=symbol,
            defaults={'company_name': COMPANY_NAMES.get(symbol, f'{symbol} Inc.')}
        )
        ticker_objects[symbol] = ticker_obj
    print(f"‚úÖ All tickers initialized")
    
    # Step 3: Fetch company news for all tickers with rate limiting
    print(f"\nüì∞ PHASE 1: Fetching company-specific news")
    print(f"   Weight in composite: {SENTIMENT_WEIGHTS['company_news']:.0%}")
    company_news_dict = fetch_company_news_batch(
        finnhub_client, 
        list(NASDAQ_TOP_20.keys()),
        lookback_hours=24
    )
    
    # Step 3.5: Check if we have any new articles
    print(f"\nüîç Checking for new articles...")
    has_new_articles = False
    all_article_hashes = []
    
    # Collect all article hashes from fetched news
    for symbol, news_articles in company_news_dict.items():
        for article in news_articles[:10]:  # Check top 10 per ticker
            article_hash = get_article_hash(
                article.get('headline', ''),
                article.get('summary', '')
            )
            all_article_hashes.append(article_hash)
    
    # Also check general market news
    market_news_preview = fetch_general_market_news(finnhub_client)
    for article in market_news_preview[:20]:  # Check top 20 market articles
        article_hash = get_article_hash(
            article.get('headline', ''),
            article.get('summary', '')
        )
        all_article_hashes.append(article_hash)
    
    # Check if any articles are new (not in database)
    if all_article_hashes:
        existing_hashes = set(
            NewsArticle.objects.filter(
                article_hash__in=all_article_hashes
            ).values_list('article_hash', flat=True)
        )
        new_hashes = set(all_article_hashes) - existing_hashes
        has_new_articles = len(new_hashes) > 0
        
        print(f"   Total articles found: {len(all_article_hashes)}")
        print(f"   Already analyzed: {len(existing_hashes)}")
        print(f"   New articles: {len(new_hashes)}")

    # Step 3.5: Fetch and analyze Reddit content (runs every time, regardless of new articles)
    print(f"\nüî¥ PHASE 4: Analyzing Reddit sentiment")
    reddit_sentiment = 0.0
    reddit_analysis_data = {}

    try:
        # Fetch Reddit posts and comments
        reddit_content = fetch_all_reddit_content()

        if reddit_content and reddit_content['posts']:
            # Analyze Reddit content
            reddit_analysis_data = analyze_reddit_content_batch(
                reddit_content['posts'],
                reddit_content['comments'],
                nasdaq_ticker
            )

            reddit_sentiment = reddit_analysis_data['composite_score']
            print(f"‚úÖ Reddit Sentiment: {reddit_sentiment:+.2f}")
            print(f"   Posts analyzed: {reddit_analysis_data.get('posts_analyzed', 0)}")
            print(f"   Comments analyzed: {reddit_analysis_data.get('comments_analyzed', 0)}")
        else:
            print(f"  ‚ö†Ô∏è  No Reddit content fetched (check API credentials)")

    except Exception as e:
        print(f"  ‚ö†Ô∏è  Reddit analysis failed: {e}")
        import traceback
        traceback.print_exc()
        reddit_sentiment = 0.0

    # If no new articles, create a duplicate run with updated price
    if not has_new_articles:
        print(f"\n‚úÖ No new articles found - creating new run with updated price")

        # Get latest analysis run
        latest_run = AnalysisRun.objects.filter(ticker=nasdaq_ticker).order_by('-timestamp').first()

        if latest_run:
            # Fetch current stock price and OHLCV from Yahoo Finance
            try:
                ohlcv = fetch_latest_ohlcv_from_yfinance(symbol='QQQ', interval='1m')

                if ohlcv:
                    new_price = Decimal(str(ohlcv['close']))
                    new_open = Decimal(str(ohlcv['open']))
                    new_high = Decimal(str(ohlcv['high']))
                    new_low = Decimal(str(ohlcv['low']))
                    new_volume = ohlcv['volume']
                    new_change = ((ohlcv['close'] - ohlcv['open']) / ohlcv['open'] * 100) if ohlcv['open'] != 0 else 0
                else:
                    # Fallback to Finnhub
                    quote = finnhub_client.quote('QQQ')
                    new_price = Decimal(str(quote['c']))
                    new_open = Decimal(str(quote.get('o', quote['c'])))
                    new_high = Decimal(str(quote.get('h', quote['c'])))
                    new_low = Decimal(str(quote.get('l', quote['c'])))
                    new_change = quote.get('dp', 0)
                    new_volume = None

                # Calculate fresh technical indicators from database history
                tech_indicators, symbol_used = fetch_indicators_with_fallback(
                    symbols=INDICATOR_SYMBOLS,
                    resolution=INDICATOR_RESOLUTION,
                    hours_back=INDICATOR_LOOKBACK_HOURS,
                    config=INDICATOR_PERIODS
                )

                # Calculate technical composite score with error handling
                try:
                    tech_composite = calculate_technical_composite_score(tech_indicators)
                    print(f"‚úÖ Technical composite calculated: {tech_composite}")
                except Exception as tech_error:
                    print(f"‚ö†Ô∏è  ERROR calculating technical composite: {tech_error}")
                    import traceback
                    traceback.print_exc()
                    tech_composite = 0.0  # Fallback to 0

                # Recalculate composite score with new technical score
                # Keep news from cache, use fresh reddit and technical
                NEWS_WEIGHT = 0.40
                REDDIT_WEIGHT = 0.30
                TECHNICAL_WEIGHT = 0.30

                # Extract news sentiment from previous run
                # The old composite was: company_news (70%) + market_news (30%)
                # We can approximate by using avg_base_sentiment as news sentiment
                # Or better: calculate from saved article data
                news_composite = latest_run.avg_base_sentiment or 0.0

                # Calculate NEW composite score with 3-way split
                new_composite_score = (
                    news_composite * NEWS_WEIGHT +
                    reddit_sentiment * REDDIT_WEIGHT +
                    tech_composite * TECHNICAL_WEIGHT
                )

                # Determine sentiment label
                if new_composite_score >= 30:
                    sentiment_label = 'BULLISH'
                elif new_composite_score <= -30:
                    sentiment_label = 'BEARISH'
                else:
                    sentiment_label = 'NEUTRAL'

                print(f"\nüìä RECALCULATED COMPOSITE SCORE (Cached Articles Path):")
                print(f"   News Sentiment:        {news_composite:+.2f} √ó {NEWS_WEIGHT:.0%} = {news_composite * NEWS_WEIGHT:+.2f}")
                print(f"   Social Media (Reddit): {reddit_sentiment:+.2f} √ó {REDDIT_WEIGHT:.0%} = {reddit_sentiment * REDDIT_WEIGHT:+.2f}")
                print(f"   Technical Indicators:  {tech_composite:+.2f} √ó {TECHNICAL_WEIGHT:.0%} = {tech_composite * TECHNICAL_WEIGHT:+.2f}")
                print(f"   Final Composite:       {new_composite_score:+.2f} ({sentiment_label})")

                # Create a new run with recalculated composite and updated price
                new_run = AnalysisRun.objects.create(
                    ticker=nasdaq_ticker,
                    composite_score=new_composite_score,
                    sentiment_label=sentiment_label,
                    stock_price=new_price,
                    price_open=new_open,
                    price_high=new_high,
                    price_low=new_low,
                    price_change_percent=new_change,
                    volume=new_volume,
                    avg_base_sentiment=latest_run.avg_base_sentiment,
                    avg_surprise_factor=latest_run.avg_surprise_factor,
                    avg_novelty=latest_run.avg_novelty,
                    avg_source_credibility=latest_run.avg_source_credibility,
                    avg_recency_weight=latest_run.avg_recency_weight,
                    articles_analyzed=latest_run.articles_analyzed,
                    cached_articles=latest_run.articles_analyzed,
                    new_articles=0,
                    # Copy technical indicators from the most recent run that has them (with MACD calculation)
                    rsi_14=tech_indicators['rsi_14'],
                    macd=tech_indicators['macd'],
                    macd_signal=tech_indicators['macd_signal'],
                    macd_histogram=tech_indicators['macd_histogram'],
                    bb_upper=tech_indicators['bb_upper'],
                    bb_middle=tech_indicators['bb_middle'],
                    bb_lower=tech_indicators['bb_lower'],
                    sma_20=tech_indicators['sma_20'],
                    sma_50=tech_indicators['sma_50'],
                    ema_9=tech_indicators['ema_9'],
                    ema_20=tech_indicators['ema_20'],
                    stoch_k=tech_indicators['stoch_k'],
                    stoch_d=tech_indicators['stoch_d'],
                    williams_r=tech_indicators['williams_r'],
                    atr_14=tech_indicators['atr_14'],
                    qqq_price=tech_indicators['qqq_price'],
                    # Reddit sentiment (fresh analysis even in cached path)
                    reddit_sentiment=reddit_sentiment,
                    reddit_posts_analyzed=reddit_analysis_data.get('posts_analyzed', 0),
                    reddit_comments_analyzed=reddit_analysis_data.get('comments_analyzed', 0),
                    # Technical composite score
                    technical_composite_score=tech_composite
                )
                
                # Copy ticker contributions from previous run
                prev_contributions = TickerContribution.objects.filter(analysis_run=latest_run)
                for contrib in prev_contributions:
                    TickerContribution.objects.create(
                        analysis_run=new_run,
                        ticker=contrib.ticker,
                        sentiment_score=contrib.sentiment_score,
                        market_cap_weight=contrib.market_cap_weight,
                        weighted_contribution=contrib.weighted_contribution,
                        articles_analyzed=contrib.articles_analyzed
                    )
                
                print(f"üìä Updated NASDAQ (QQQ) Price: ${new_price:.2f} ({new_change:+.2f}%)")
                print(f"‚úÖ Created new run #{new_run.id} with updated price (sentiment unchanged)")
                print(f"‚è±Ô∏è  Completed in {time.time() - start_time:.1f} seconds")
                
                return new_run
            except Exception as e:
                print(f"‚ö†Ô∏è Error creating new run: {e}")
                return None
        else:
            print(f"‚ö†Ô∏è No previous analysis found - running full analysis")
            has_new_articles = True  # Force full analysis
    
    # Continue with full analysis if we have new articles
    print(f"\nüî¨ Proceeding with full sentiment analysis...")
    
    # Step 4: Analyze sentiment for each ticker
    print(f"\nüîç PHASE 2: Analyzing company sentiment")
    ticker_sentiments = {}
    ticker_contributions = {}
    all_company_articles = []
    
    for symbol, weight in NASDAQ_TOP_20.items():
        print(f"\n  Analyzing {symbol} ({weight:.1%} weight)...")
        ticker_obj = ticker_objects[symbol]
        news_articles = company_news_dict.get(symbol, [])
        
        if not news_articles:
            print(f"    ‚ö†Ô∏è No news found")
            ticker_sentiments[symbol] = 0.0
            ticker_contributions[symbol] = {
                'sentiment': 0.0,
                'weight': weight,
                'contribution': 0.0,
                'articles_count': 0,
                'articles_data': []
            }
            continue
        
        # Batch analyze articles (top 10 most recent)
        articles_to_process = news_articles[:10]
        articles_data = analyze_articles_batch(articles_to_process, ticker_obj, article_type='company')
        
        # Calculate total score
        total_score = sum(article_data['article_score'] for article_data in articles_data)
        all_company_articles.extend(articles_data)
        
        avg_sentiment = total_score / len(articles_data) if articles_data else 0
        weighted_contribution = avg_sentiment * weight
        
        ticker_sentiments[symbol] = avg_sentiment
        ticker_contributions[symbol] = {
            'sentiment': avg_sentiment,
            'weight': weight,
            'contribution': weighted_contribution,
            'articles_count': len(articles_data),
            'articles_data': articles_data
        }
        
        print(f"    ‚úì Sentiment: {avg_sentiment:+.2f} | Contribution: {weighted_contribution:+.2f} | Articles: {len(articles_data)}")
    
    # Step 5: Calculate weighted company sentiment
    company_sentiment = sum(
        contrib['contribution'] 
        for contrib in ticker_contributions.values()
    )
    
    print(f"\nüìà Company News Composite Sentiment: {company_sentiment:+.2f}")
    
    # Step 6: Fetch and analyze general market news (reuse if already fetched)
    print(f"\nüì° PHASE 3: Analyzing general market news")
    print(f"   Weight in composite: {SENTIMENT_WEIGHTS['market_news']:.0%}")
    
    # Use the market news we already fetched during the check, or fetch fresh if needed
    market_news = market_news_preview if 'market_news_preview' in locals() else fetch_general_market_news(finnhub_client)
    market_articles_data = []
    market_sentiment = 0.0
    
    if market_news:
        # Batch analyze market news (top 20 most relevant)
        print(f"  üìä Processing {min(20, len(market_news))} market news articles...")
        market_articles_data = analyze_articles_batch(market_news[:20], nasdaq_ticker, article_type='market')
        
        total_market_score = sum(article_data['article_score'] for article_data in market_articles_data)
        market_sentiment = total_market_score / len(market_articles_data) if market_articles_data else 0
        print(f"‚úÖ Market News Sentiment: {market_sentiment:+.2f} | Articles: {len(market_articles_data)}")
    else:
        print(f"‚ö†Ô∏è No relevant market news found")

    # Step 7: Calculate news composite (company + market)
    news_composite = (
        company_sentiment * SENTIMENT_WEIGHTS['company_news'] +
        market_sentiment * SENTIMENT_WEIGHTS['market_news']
    )
    
    # Step 8: Get NASDAQ index price and OHLCV data from Yahoo Finance (using QQQ as proxy)
    try:
        print(f"\nüìä Fetching real-time OHLCV from Yahoo Finance...")
        ohlcv = fetch_latest_ohlcv_from_yfinance(symbol='QQQ', interval='1m')

        if ohlcv:
            index_price = Decimal(str(ohlcv['close']))
            price_open = Decimal(str(ohlcv['open']))
            price_high = Decimal(str(ohlcv['high']))
            price_low = Decimal(str(ohlcv['low']))
            volume = ohlcv['volume']

            # Calculate price change percentage
            price_change = ((ohlcv['close'] - ohlcv['open']) / ohlcv['open'] * 100) if ohlcv['open'] != 0 else 0
        else:
            raise Exception("No OHLCV data from Yahoo Finance")

    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not fetch OHLCV from Yahoo Finance: {e}")
        print(f"  Falling back to Finnhub quote...")
        try:
            quote = finnhub_client.quote('QQQ')
            index_price = Decimal(str(quote['c']))
            price_open = Decimal(str(quote.get('o', quote['c'])))
            price_high = Decimal(str(quote.get('h', quote['c'])))
            price_low = Decimal(str(quote.get('l', quote['c'])))
            price_change = quote.get('dp', 0)
            volume = None
        except Exception as e2:
            print(f"\n‚ö†Ô∏è Finnhub fallback also failed: {e2}")
            index_price = Decimal('0.00')
            price_open = Decimal('0.00')
            price_high = Decimal('0.00')
            price_low = Decimal('0.00')
            price_change = 0.0
            volume = None
    
    # Step 9: Calculate technical indicators from OHLCV data
    # Fetch fresh OHLCV data and calculate indicators on every run
    technical_indicators, symbol_used = fetch_indicators_with_fallback(
        symbols=INDICATOR_SYMBOLS,
        resolution=INDICATOR_RESOLUTION,
        hours_back=INDICATOR_LOOKBACK_HOURS,
        config=INDICATOR_PERIODS
    )

    # Step 9.5: Calculate technical composite score
    technical_composite_score = calculate_technical_composite_score(technical_indicators)
    print(f"\nüìä Technical Indicators Composite Score: {technical_composite_score:+.2f}")

    # Step 10: Calculate FINAL composite score (News 40% + Reddit 30% + Technical 30%)
    NEWS_WEIGHT = 0.40
    REDDIT_WEIGHT = 0.30
    TECHNICAL_WEIGHT = 0.30

    final_composite_score = (
        news_composite * NEWS_WEIGHT +
        reddit_sentiment * REDDIT_WEIGHT +
        technical_composite_score * TECHNICAL_WEIGHT
    )

    print(f"\n{'='*80}")
    print(f"üéØ FINAL NASDAQ COMPOSITE SENTIMENT SCORE: {final_composite_score:+.2f}")
    print(f"{'='*80}")
    print(f"   News Sentiment:        {news_composite:+.2f} √ó {NEWS_WEIGHT:.0%} = {news_composite * NEWS_WEIGHT:+.2f}")
    print(f"   Social Media (Reddit): {reddit_sentiment:+.2f} √ó {REDDIT_WEIGHT:.0%} = {reddit_sentiment * REDDIT_WEIGHT:+.2f}")
    print(f"   Technical Indicators:  {technical_composite_score:+.2f} √ó {TECHNICAL_WEIGHT:.0%} = {technical_composite_score * TECHNICAL_WEIGHT:+.2f}")
    print(f"{'='*80}")

    # Step 11: Save to database
    print(f"\nüíæ Saving analysis to database...")

    with transaction.atomic():
        # Calculate averages for component scores
        all_articles = all_company_articles + market_articles_data
        total_articles = len(all_articles)
        cached_count = sum(1 for a in all_articles if a['is_cached'])
        new_count = total_articles - cached_count

        avg_base_sentiment = sum(a['base_sentiment'] for a in all_articles) / total_articles if total_articles else 0
        avg_surprise = sum(a['surprise_factor'] for a in all_articles) / total_articles if total_articles else 1.0
        avg_novelty = sum(a['novelty_score'] for a in all_articles) / total_articles if total_articles else 1.0
        avg_credibility = sum(a['source_credibility'] for a in all_articles) / total_articles if total_articles else 0.5
        avg_recency = sum(a['recency_weight'] for a in all_articles) / total_articles if total_articles else 0.5

        # Create AnalysisRun with OHLCV and technical indicators
        analysis_run = AnalysisRun.objects.create(
            ticker=nasdaq_ticker,
            composite_score=float(final_composite_score),
            avg_base_sentiment=avg_base_sentiment,
            avg_surprise_factor=avg_surprise,
            avg_novelty=avg_novelty,
            avg_source_credibility=avg_credibility,
            avg_recency_weight=avg_recency,
            # OHLCV data (from Yahoo Finance)
            stock_price=index_price,
            price_open=price_open,
            price_high=price_high,
            price_low=price_low,
            price_change_percent=price_change,
            volume=volume,
            articles_analyzed=total_articles,
            cached_articles=cached_count,
            new_articles=new_count,
            # Technical indicators
            rsi_14=technical_indicators.get('rsi_14'),
            macd=technical_indicators.get('macd'),
            macd_signal=technical_indicators.get('macd_signal'),
            macd_histogram=technical_indicators.get('macd_histogram'),
            bb_upper=technical_indicators.get('bb_upper'),
            bb_middle=technical_indicators.get('bb_middle'),
            bb_lower=technical_indicators.get('bb_lower'),
            sma_20=technical_indicators.get('sma_20'),
            sma_50=technical_indicators.get('sma_50'),
            ema_9=technical_indicators.get('ema_9'),
            ema_20=technical_indicators.get('ema_20'),
            stoch_k=technical_indicators.get('stoch_k'),
            stoch_d=technical_indicators.get('stoch_d'),
            williams_r=technical_indicators.get('williams_r'),
            atr_14=technical_indicators.get('atr_14'),
            qqq_price=technical_indicators.get('qqq_price'),
            # Reddit sentiment
            reddit_sentiment=reddit_sentiment,
            reddit_posts_analyzed=reddit_analysis_data.get('posts_analyzed', 0),
            reddit_comments_analyzed=reddit_analysis_data.get('comments_analyzed', 0),
            # Technical composite score
            technical_composite_score=technical_composite_score
        )
        
        print(f"‚úì Created AnalysisRun #{analysis_run.id}")
        
        # Save ticker contributions
        for symbol, contrib in ticker_contributions.items():
            if contrib['articles_count'] > 0:
                TickerContribution.objects.create(
                    analysis_run=analysis_run,
                    ticker=ticker_objects[symbol],
                    sentiment_score=contrib['sentiment'],
                    market_cap_weight=contrib['weight'],
                    weighted_contribution=contrib['contribution'],
                    articles_analyzed=contrib['articles_count']
                )
        
        print(f"‚úì Saved {len(ticker_contributions)} ticker contributions")
        
        # Save all articles
        for article_data in all_articles:
            NewsArticle.objects.update_or_create(
                article_hash=article_data['article_hash'],
                defaults={
                    'ticker': nasdaq_ticker if article_data['article_type'] == 'market' else ticker_objects.get(article_data.get('ticker_symbol', '^IXIC'), nasdaq_ticker),
                    'analysis_run': analysis_run,
                    'headline': article_data['headline'],
                    'summary': article_data['summary'],
                    'source': article_data['source'],
                    'url': article_data['url'],
                    'published_at': article_data['published_at'],
                    'article_type': article_data['article_type'],
                    'base_sentiment': article_data['base_sentiment'],
                    'surprise_factor': article_data['surprise_factor'],
                    'novelty_score': article_data['novelty_score'],
                    'source_credibility': article_data['source_credibility'],
                    'recency_weight': article_data['recency_weight'],
                    'article_score': article_data['article_score'],
                    'weighted_contribution': article_data['article_score'] / total_articles if total_articles else 0,
                    'is_analyzed': True,
                    'sentiment_cached': article_data['is_cached']
                }
            )
        
        print(f"‚úì Saved {total_articles} news articles")
    
    elapsed_time = time.time() - start_time
    print(f"\n‚úÖ Analysis complete in {elapsed_time:.1f} seconds")
    print(f"üìä View results: http://localhost:8000/admin/api/analysisrun/{analysis_run.id}/change/")
    
    return analysis_run


# ============================================================================
# DJANGO COMMAND
# ============================================================================

class Command(BaseCommand):
    help = 'Run NASDAQ composite sentiment analysis using hybrid approach'
    
    def add_arguments(self, parser):
        parser.add_argument(
            '--interval',
            type=int,
            default=300,
            help='Update interval in seconds (default: 300 = 5 minutes)'
        )
        parser.add_argument(
            '--once',
            action='store_true',
            help='Run analysis once and exit (don\'t monitor continuously)'
        )
    
    def handle(self, *args, **options):
        interval = options['interval']
        run_once = options['once']
        
        # Validate API keys
        if not FINNHUB_API_KEY:
            self.stdout.write(self.style.ERROR(
                '‚ùå FINNHUB_API_KEY not set! Set it in .env file.'
            ))
            return
        
        if not HUGGINGFACE_API_KEY:
            self.stdout.write(self.style.ERROR(
                '‚ùå HUGGINGFACE_API_KEY not set! Set it in .env file.'
            ))
            return
        
        # Initialize Finnhub client
        finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)
        
        self.stdout.write(self.style.SUCCESS(
            '\nüöÄ NASDAQ Composite Sentiment Tracker - Hybrid Approach'
        ))
        self.stdout.write(f'üìä Tracking {len(NASDAQ_TOP_20)} top NASDAQ stocks')
        self.stdout.write(f'üîÑ Company News Weight: {SENTIMENT_WEIGHTS["company_news"]:.0%}')
        self.stdout.write(f'üì° Market News Weight: {SENTIMENT_WEIGHTS["market_news"]:.0%}')
        self.stdout.write('üíæ All data saved to database\n')
        
        if run_once:
            # Run once
            result = run_nasdaq_composite_analysis(finnhub_client)
            if result:
                # Handle both dict (price-only update) and AnalysisRun object
                if isinstance(result, dict):
                    self.stdout.write(f"\nüìä Current Score: {result['composite_score']:+.2f}")
                    self.stdout.write(f"üí∞ Stock Price: ${result['stock_price']:.2f} ({result['price_change']:+.2f}%)")
                    self.interpret_score(result['composite_score'])
                else:
                    self.interpret_score(result.composite_score)
        else:
            # Run continuously
            self.stdout.write(f'‚è±Ô∏è  Update interval: {interval} seconds')
            self.stdout.write('‚å®Ô∏è  Press Ctrl+C to stop\n')
            
            iteration = 0
            
            try:
                while True:
                    iteration += 1
                    self.stdout.write(f"\n{'>'*30} ITERATION {iteration} {'<'*30}")
                    
                    result = run_nasdaq_composite_analysis(finnhub_client)
                    if result:
                        # Handle both dict (price-only update) and AnalysisRun object
                        if isinstance(result, dict):
                            self.stdout.write(f"\nüìä Current Score: {result['composite_score']:+.2f}")
                            self.stdout.write(f"üí∞ Stock Price: ${result['stock_price']:.2f} ({result['price_change']:+.2f}%)")
                            self.interpret_score(result['composite_score'])
                        else:
                            self.interpret_score(result.composite_score)
                    
                    self.stdout.write(f"\nüí§ Next update in {interval} seconds...")
                    time.sleep(interval)
                    
            except KeyboardInterrupt:
                self.stdout.write(self.style.WARNING(
                    f'\n\n‚õî Monitoring stopped by user.'
                ))
                self.stdout.write(f'üìä Total iterations: {iteration}')
                self.stdout.write(f'üì∞ Unique articles seen: {len(seen_articles)}')
    
    def interpret_score(self, score):
        """Interpret and display the sentiment score"""
        if score > 50:
            sentiment = self.style.SUCCESS("STRONGLY BULLISH üöÄ")
        elif score > 20:
            sentiment = self.style.SUCCESS("BULLISH üìà")
        elif score > -20:
            sentiment = self.style.WARNING("NEUTRAL ‚û°Ô∏è")
        elif score > -50:
            sentiment = self.style.ERROR("BEARISH üìâ")
        else:
            sentiment = self.style.ERROR("STRONGLY BEARISH üîª")
        
        self.stdout.write(f"\n{'='*80}")
        self.stdout.write(f"Current NASDAQ Sentiment: {sentiment}")
        self.stdout.write(f"Score: {score:+.2f}/100")
        self.stdout.write(f"{'='*80}")

